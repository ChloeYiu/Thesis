\begin{center}
    \Large
    \textbf{Bias and Fairness in Automatic Spoken Language Assessment}
    \vspace{0.2in} \\

    \normalsize
    \textbf{Chloe Yee Man Yiu} \\
    Lucy Cavendish College \\
    Supervised by Dr Kate Knill
    \vspace{0.1in}

    June 2025
\end{center}

\section*{Technical Abstract}
The adoption of automated systems for spoken language assessment helps satisfy the increasing demand for English speaking assessment. The advancement in AI-powered technologies have significantly enhanced the accuracy of automated scoring. However, such systems may exhibit biases toward or against certain user groups. The cause of biases could due to imbalanced training data, biases in human annotations, or inherent flaws within the model architecture. An ideal automatic spoken language assessment (SLA) system should evaluate a candidate's language proficiency independently of factors like their first language (L1), gender, or age.

This research focuses on a novel method for detecting bias in SLA models using Concept Activation Vectors (CAVs). CAVs enable an interpretable analysis of whether models involving neural networks are influenced by specific concepts. By examining model activations, we can pinpoint instances where non-linguistic attributes may unintentionally affect scoring. This facilitates the targeted collection of relevant data to validate these findings, reducing the time and resources required for bias detection.

This methodology is applied to various SLA models, including those utilizing features, text, and audio, to evaluate how model selection influences bias levels. The analysis investigates the relationship between the characteristics of CAVs and the sensitivity to bias, revealing that neither the accuracy of CAVs nor the weighting of data during extraction correlates with a CAV's bias sensitivity. Further exploration into factors affecting patterns observed in CAV analysis is done. Although the evidence is generally inconclusive in the factors affecting the bias sensitivity, it is found that the activation function has significant influence on the patterns. These findings emphasize the critical importance of fairness-aware AI development and demonstrate how bias analysis using the CAV approach can contribute to the creation of more equitable and reliable AI-driven assessment systems.

\clearpage
